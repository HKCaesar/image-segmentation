{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous post, we implemented the upsampling and made sure it is correct\n",
    "by comparing it to the implementation of the [scikit-image library](http://scikit-image.org/).\n",
    "To be more specific we had _FCN-32_ _Segmentation_ network implemented which is\n",
    "described in the paper _Fully convolutional networks for semantic segmentation_.\n",
    "\n",
    "In this post we will perform a simple training: we will get a sample image from\n",
    "[PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) dataset along with annotation,\n",
    "train our network on them and test our network on the same image. It was done this way\n",
    "so that it can also be run on CPU -- it takes only 10 iterations for the training to complete.\n",
    "Another point of this post is to show that segmentation that our network (FCN-32s) produces is\n",
    "very coarse -- even if we run it on the same image that we were training it on. In this post\n",
    "we tackle this problem by performing Conditional Random Field post-processing stage, which\n",
    "refines our segmentation by taking into account pure RGB features of image and probabilities\n",
    "produced by our network. Overall, we get a refined segmentation. The set-up of this post\n",
    "is very simple on purpose. Similar approach to Segmentation was described in the paper\n",
    "_Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs_ by Chen et al.\n",
    "\n",
    "The blog post is created using jupyter notebook. After each chunk of a code\n",
    "you can see the result of its evaluation. You can also get the notebook\n",
    "file from [here](http://google.com). The content of the blog post\n",
    "is partially borrowed from [slim walkthough notebook](https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to run the code, you will need to have Tensorflow installed. I have used _r0.12_.\n",
    "You will need to use [this fork of _tensorflow/models_](https://github.com/tensorflow/models/pull/684). \n",
    "\n",
    "I am also using scikit-image library and numpy for this tutorial plus other\n",
    "dependencies. One of the ways to install them is to download _Anaconda_ software\n",
    "package for python.\n",
    "\n",
    "Follow all the other steps described in the previous posts -- it shows how to download\n",
    "the _VGG-16_ model and I also forked the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "from nets import vgg\n",
    "from preprocessing import vgg_preprocessing\n",
    "from libs.scale_input_image import scale_randomly_image_with_annotation_with_fixed_size_output\n",
    "from libs.training import get_valid_logits_and_labels\n",
    "from libs.training import get_labels_from_annotation\n",
    "# Load the mean pixel values and the function\n",
    "# that performs the subtraction from each pixel\n",
    "from preprocessing.vgg_preprocessing import (_mean_image_subtraction,\n",
    "                                            _R_MEAN, _G_MEAN, _B_MEAN)\n",
    "import sys\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0:   background\n",
    "#1:   aeroplane\n",
    "#2:   bicycle\n",
    "#3:   bird\n",
    "#4:   boat\n",
    "#5:   bottle\n",
    "#6:   bus\n",
    "#7:   car\n",
    "#8:   cat\n",
    "#9:   chair\n",
    "#10:  cow\n",
    "#11:  diningtable\n",
    "#12:  dog\n",
    "#13:  horse\n",
    "#14:  motorbike\n",
    "#15:  person\n",
    "#16:  pottedplant\n",
    "#17:  sheep\n",
    "#18:  sofa\n",
    "#19:  train\n",
    "#20:  tvmonitor\n",
    "#255: undefined/don't care\n",
    "number_of_classes = 21\n",
    "class_labels = [v for v in range((number_of_classes+1))]\n",
    "class_labels[-1] = 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling helper functions and Image Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we define helper functions that were used in previous post.\n",
    "If you recall, we used upsampling to upsample the downsampled predictions\n",
    "that we get from our network. We get downsampled predictions because of max-pooling\n",
    "layers that are used in _VGG-16_ network.\n",
    "\n",
    "We also write code for image and respective ground-truth segmentation loading.\n",
    "The code is well-commented, so don't be afraid to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_kernel_size(factor):\n",
    "    \"\"\"\n",
    "    Find the kernel size given the desired factor of upsampling.\n",
    "    \"\"\"\n",
    "    return 2 * factor - factor % 2\n",
    "\n",
    "\n",
    "def upsample_filt(size):\n",
    "    \"\"\"\n",
    "    Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "    \"\"\"\n",
    "    factor = (size + 1) // 2\n",
    "    if size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:size, :size]\n",
    "    return (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "\n",
    "def bilinear_upsample_weights(factor, number_of_classes):\n",
    "    \"\"\"\n",
    "    Create weights matrix for transposed convolution with bilinear filter\n",
    "    initialization.\n",
    "    \"\"\"\n",
    "    \n",
    "    filter_size = get_kernel_size(factor)\n",
    "    \n",
    "    weights = np.zeros((filter_size,\n",
    "                        filter_size,\n",
    "                        number_of_classes,\n",
    "                        number_of_classes), dtype=np.float32)\n",
    "    \n",
    "    upsample_kernel = upsample_filt(filter_size)\n",
    "    \n",
    "    for i in range(number_of_classes):\n",
    "        \n",
    "        weights[:, :, i, i] = upsample_kernel\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_filenames = \"/home/thalles_silva/DataPublic/PascalVoc2012/train/VOC2012/ImageSets/Segmentation/train.txt\"\n",
    "training_dir = \"/home/thalles_silva/DataPublic/PascalVoc2012/train/VOC2012/JPEGImages\"\n",
    "annotations_dir = \"/home/thalles_silva/DataPublic/PascalVoc2012/train/VOC2012/SegmentationClass_1D\"\n",
    "log_folder = '/home/thalles_silva/Thalles/image-segmentation/log_folder'\n",
    "checkpoints_dir = '/home/thalles_silva/Thalles/image-segmentation/vgg'\n",
    "vgg_checkpoint_path = os.path.join(checkpoints_dir, 'vgg_16.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_input():\n",
    "    is_training_placeholder = tf.placeholder(tf.bool)\n",
    "    return is_training_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 21, 21)\n"
     ]
    }
   ],
   "source": [
    "upsample_filter_factor_2_np = bilinear_upsample_weights(factor=2,\n",
    "                                                        number_of_classes=number_of_classes)\n",
    "print(upsample_filter_factor_2_np.shape)\n",
    "upsample_filter_factor_8_np = bilinear_upsample_weights(factor=4,\n",
    "                                                         number_of_classes=number_of_classes)\n",
    "\n",
    "upsample_filter_factor_2_tensor = tf.constant(upsample_filter_factor_2_np)\n",
    "upsample_filter_factor_8_tensor = tf.constant(upsample_filter_factor_8_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(processed_images, number_of_classes=21, is_training=True):\n",
    "\n",
    "    with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "\n",
    "        _, end_points = vgg.vgg_16(processed_images,\n",
    "                                                   num_classes=number_of_classes,\n",
    "                                                   is_training=is_training,\n",
    "                                                   spatial_squeeze=False,\n",
    "                                                   fc_conv_padding='SAME')\n",
    "\n",
    "\n",
    "    \n",
    "    # get the vggs pool5 feature map, this way we do not use the last layer of the vgg net therefore making the net faster\n",
    "    pool5_feature_map = end_points['vgg_16/pool5']\n",
    "\n",
    "    pool5_logits = slim.conv2d(pool5_feature_map,\n",
    "                               number_of_classes,\n",
    "                               [1, 1],\n",
    "                               activation_fn=None,\n",
    "                               normalizer_fn=None,\n",
    "                               scope=\"seg_vars/pool5\",\n",
    "                               weights_initializer=tf.zeros_initializer) # Out: # (1, 22, 30, 2)\n",
    "    \n",
    "    pool5_layer_logits_shape = tf.shape(pool5_logits)\n",
    "    \n",
    "    # Calculate the ouput size of the upsampled tensor\n",
    "    last_layer_upsampled_by_factor_2_logits_shape = tf.stack([\n",
    "                                                          pool5_layer_logits_shape[0],\n",
    "                                                          pool5_layer_logits_shape[1] * 2,\n",
    "                                                          pool5_layer_logits_shape[2] * 2,\n",
    "                                                          pool5_layer_logits_shape[3]\n",
    "                                                         ])\n",
    "\n",
    "    # Perform the upsampling\n",
    "    last_layer_upsampled_by_factor_2_logits = tf.nn.conv2d_transpose(pool5_logits,\n",
    "                                                                     upsample_filter_factor_2_tensor,\n",
    "                                                                     output_shape=last_layer_upsampled_by_factor_2_logits_shape,\n",
    "                                                                     strides=[1, 2, 2, 1])\n",
    "\n",
    "    ## Adding the skip here for FCN-16s model\n",
    "\n",
    "    # We created vgg in the fcn_8s name scope -- so\n",
    "    # all the vgg endpoints now are prepended with fcn_8s name\n",
    "    pool4_features = end_points['vgg_16/pool4']\n",
    "\n",
    "    # We zero initialize the weights to start training with the same\n",
    "    # accuracy that we ended training FCN-32s\n",
    "\n",
    "    pool4_logits = slim.conv2d(pool4_features,\n",
    "                               number_of_classes,\n",
    "                               [1, 1],\n",
    "                               activation_fn=None,\n",
    "                               normalizer_fn=None,\n",
    "                               weights_initializer=tf.zeros_initializer,\n",
    "                               scope='seg_vars/pool4')\n",
    "\n",
    "    fused_last_layer_and_pool4_logits = pool4_logits + last_layer_upsampled_by_factor_2_logits\n",
    "\n",
    "    fused_last_layer_and_pool4_logits_shape = tf.shape(fused_last_layer_and_pool4_logits)\n",
    "\n",
    "    # Calculate the ouput size of the upsampled tensor\n",
    "    fused_last_layer_and_pool4_upsampled_by_factor_2_logits_shape = tf.stack([\n",
    "                                                                  fused_last_layer_and_pool4_logits_shape[0],\n",
    "                                                                  fused_last_layer_and_pool4_logits_shape[1] * 2,\n",
    "                                                                  fused_last_layer_and_pool4_logits_shape[2] * 2,\n",
    "                                                                  fused_last_layer_and_pool4_logits_shape[3]\n",
    "                                                                 ])\n",
    "\n",
    "    # Perform the upsampling\n",
    "    fused_last_layer_and_pool4_upsampled_by_factor_2_logits = tf.nn.conv2d_transpose(fused_last_layer_and_pool4_logits,\n",
    "                                                                upsample_filter_factor_2_tensor,\n",
    "                                                                output_shape=fused_last_layer_and_pool4_upsampled_by_factor_2_logits_shape,\n",
    "                                                                strides=[1, 2, 2, 1])\n",
    "\n",
    "\n",
    "    ## Adding the skip here for FCN-8s model\n",
    "    pool3_features = end_points['vgg_16/pool3']\n",
    "\n",
    "    # We zero initialize the weights to start training with the same\n",
    "    # accuracy that we ended training FCN-32s\n",
    "\n",
    "    pool3_logits = slim.conv2d(pool3_features,\n",
    "                               number_of_classes,\n",
    "                               [1, 1],\n",
    "                               activation_fn=None,\n",
    "                               normalizer_fn=None,\n",
    "                               weights_initializer=tf.zeros_initializer,\n",
    "                               scope='seg_vars/pool3')\n",
    "\n",
    "\n",
    "    fused_last_layer_and_pool4_logits_and_pool_3_logits = pool3_logits + \\\n",
    "                                    fused_last_layer_and_pool4_upsampled_by_factor_2_logits\n",
    "\n",
    "\n",
    "    fused_last_layer_and_pool4_logits_and_pool_3_logits_shape = tf.shape(fused_last_layer_and_pool4_logits_and_pool_3_logits)\n",
    "\n",
    "\n",
    "    # Calculate the ouput size of the upsampled tensor\n",
    "    fused_last_layer_and_pool4_logits_and_pool_3_upsampled_by_factor_8_logits_shape = tf.stack([\n",
    "                                                                  fused_last_layer_and_pool4_logits_and_pool_3_logits_shape[0],\n",
    "                                                                  fused_last_layer_and_pool4_logits_and_pool_3_logits_shape[1] * 8,\n",
    "                                                                  fused_last_layer_and_pool4_logits_and_pool_3_logits_shape[2] * 8,\n",
    "                                                                  fused_last_layer_and_pool4_logits_and_pool_3_logits_shape[3]\n",
    "                                                                 ])\n",
    "\n",
    "    # Perform the upsampling\n",
    "    fused_last_layer_and_pool4_logits_and_pool_3_upsampled_by_factor_8_logits = tf.nn.conv2d_transpose(fused_last_layer_and_pool4_logits_and_pool_3_logits,\n",
    "                                                                upsample_filter_factor_8_tensor,\n",
    "                                                                output_shape=fused_last_layer_and_pool4_logits_and_pool_3_upsampled_by_factor_8_logits_shape,\n",
    "                                                                strides=[1, 8, 8, 1])\n",
    "    \n",
    "    \n",
    "\n",
    "    return fused_last_layer_and_pool4_logits_and_pool_3_upsampled_by_factor_8_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_loss(upsampled_by_factor_16_logits, labels):\n",
    "    #labels = tf.squeeze(labels)\n",
    "    #valid_labels_batch_tensor, valid_logits_batch_tensor = get_valid_logits_and_labels(annotation_batch_tensor=labels,\n",
    "    #                                                                                   logits_batch_tensor=upsampled_by_factor_16_logits,\n",
    "    #                                                                                   class_labels=class_labels)\n",
    "    \n",
    "\n",
    "\n",
    "    #cross_entropies = tf.nn.softmax_cross_entropy_with_logits(logits=valid_logits_batch_tensor,\n",
    "    #                                                          labels=valid_labels_batch_tensor)\n",
    "    \n",
    "    cross_entropies = tf.nn.softmax_cross_entropy_with_logits(logits=upsampled_by_factor_16_logits,\n",
    "                                                              labels=labels)\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropies)\n",
    "    \n",
    "    # Add summary op for the loss -- to be able to see it in tensorboard.\n",
    "    tf.summary.scalar('cross_entropy_loss', cross_entropy_mean)\n",
    "\n",
    "    # Tensor to get the final prediction for each pixel -- pay \n",
    "    # attention that we don't need softmax in this case because\n",
    "    # we only need the final decision. If we also need the respective\n",
    "    # probabilities we will have to apply softmax.\n",
    "    pred = tf.argmax(upsampled_by_factor_16_logits, dimension=3)\n",
    "    probabilities = tf.nn.softmax(upsampled_by_factor_16_logits)\n",
    "    \n",
    "    return cross_entropy_mean, pred, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_optimizer(cross_entropy_sum, learning_rate):\n",
    "    with tf.variable_scope(\"adam_vars\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy_sum)\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(training_filenames, 'r')\n",
    "images_filenale_list = [line for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an image for testing the network\n",
    "image_path = training_dir + \"/2008_004453.jpg\"\n",
    "annotation_path = annotations_dir + \"/2008_004453.png\"\n",
    "\n",
    "image_tensor = tf.read_file(image_path)\n",
    "image_tensor = tf.image.decode_jpeg(image_tensor, channels=3)\n",
    "image_tensor = tf.cast(image_tensor, tf.float32)\n",
    "image_tensor = _mean_image_subtraction(image_tensor,[_R_MEAN, _G_MEAN, _B_MEAN])\n",
    "image_tensor = tf.expand_dims(image_tensor, axis=0) # (1, ?, ?, 3) # BATCH,WIDTH,HEIGHTxDEPTH\n",
    "\n",
    "annotation_tensor = tf.read_file(annotation_path)\n",
    "annotation_tensor = tf.image.decode_png(annotation_tensor, channels=1)\n",
    "annotation_masks_tensor = get_labels_from_annotation(tf.squeeze(annotation_tensor), class_labels)\n",
    "annotation_masks_tensor = tf.expand_dims(annotation_masks_tensor, axis=0) # BATCH,WIDTH,HEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training_placeholder = model_input()\n",
    "\n",
    "upsampled_by_factor_16_logits = model(image_tensor, number_of_classes=number_of_classes, is_training=True)\n",
    "\n",
    "cross_entropy_sum, pred, probabilities = model_loss(upsampled_by_factor_16_logits, annotation_masks_tensor)\n",
    "\n",
    "train_step = model_optimizer(cross_entropy_sum, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the accuracy metric: Mean Intersection Over Union\n",
    "miou, update_op = slim.metrics.streaming_mean_iou(predictions=pred,\n",
    "                                                   labels=annotation_tensor,\n",
    "                                                   num_classes=number_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all segmentation model vars, these are the variables we create to perform \n",
    "# the segmentation upsampling layers\n",
    "model_variables = [ var.op.name for var in slim.get_variables(scope=\"seg_vars\") ]\n",
    "\n",
    "# Now we define a function that will load the weights from VGG checkpoint\n",
    "# into our variables when we call it. We exclude the weights from the last layer\n",
    "# which is responsible for class predictions. We do this because \n",
    "# we will have different number of classes to predict and we can't\n",
    "# use the old ones as an initialization.\n",
    "exclude_vars = model_variables + ['vgg_16/fc8', 'adam_vars']\n",
    "vgg_except_fc8_weights = slim.get_variables_to_restore(exclude=exclude_vars)\n",
    "\n",
    "# Here we get variables that belong to the last layer of network.\n",
    "# As we saw, the number of classes that VGG was originally trained on\n",
    "# is different from ours -- in our case it is only 2 classes.\n",
    "vgg_fc8_weights = slim.get_variables_to_restore(include=['vgg_16/fc8'])\n",
    "\n",
    "adam_optimizer_variables = slim.get_variables_to_restore(include=['adam_vars'])\n",
    "\n",
    "# get the segmentation upsampling variables to be initialized \n",
    "model_variables = slim.get_variables(scope=\"seg_vars\")\n",
    "\n",
    "# Put all summary ops into one op. Produces string when you run it.\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "# Create the summary writer -- to write all the logs\n",
    "# into a specified file. This file can be later read\n",
    "# by tensorboard.\n",
    "summary_string_writer = tf.summary.FileWriter(log_folder)\n",
    "\n",
    "# Create the log folder if doesn't exist yet\n",
    "if not os.path.exists(log_folder):\n",
    "    os.makedirs(log_folder)\n",
    "\n",
    "# Create an OP that performs the initialization of\n",
    "# the VGG net variables.\n",
    "read_vgg_weights_except_fc8_func = slim.assign_from_checkpoint_fn(\n",
    "                                   vgg_checkpoint_path,\n",
    "                                   vgg_except_fc8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/thalles_silva/Thalles/image-segmentation/vgg/vgg_16.ckpt\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [1,31,23,21] vs. [1,30,22,21]\n\t [[Node: add = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](seg_vars/pool4/BiasAdd, conv2d_transpose)]]\n\t [[Node: ResizeBilinear/_65 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_331_ResizeBilinear\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'add', defined at:\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-b8cf0e0e9281>\", line 3, in <module>\n    upsampled_by_factor_16_logits = model(image_tensor, number_of_classes=number_of_classes, is_training=True)\n  File \"<ipython-input-7-30438eaf8d01>\", line 58, in model\n    fused_last_layer_and_pool4_logits = pool4_logits + last_layer_upsampled_by_factor_2_logits\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 821, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 73, in add\n    result = _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [1,31,23,21] vs. [1,30,22,21]\n\t [[Node: add = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](seg_vars/pool4/BiasAdd, conv2d_transpose)]]\n\t [[Node: ResizeBilinear/_65 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_331_ResizeBilinear\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [1,31,23,21] vs. [1,30,22,21]\n\t [[Node: add = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](seg_vars/pool4/BiasAdd, conv2d_transpose)]]\n\t [[Node: ResizeBilinear/_65 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_331_ResizeBilinear\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-14f9d81ac7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     out, train_image, train_annotation = sess.run([upsampled_by_factor_16_logits, image_tensor, annotation_tensor],\n\u001b[0;32m---> 22\u001b[0;31m                                               feed_dict={is_training_placeholder: False})\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [1,31,23,21] vs. [1,30,22,21]\n\t [[Node: add = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](seg_vars/pool4/BiasAdd, conv2d_transpose)]]\n\t [[Node: ResizeBilinear/_65 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_331_ResizeBilinear\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'add', defined at:\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-b8cf0e0e9281>\", line 3, in <module>\n    upsampled_by_factor_16_logits = model(image_tensor, number_of_classes=number_of_classes, is_training=True)\n  File \"<ipython-input-7-30438eaf8d01>\", line 58, in model\n    fused_last_layer_and_pool4_logits = pool4_logits + last_layer_upsampled_by_factor_2_logits\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 821, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 73, in add\n    result = _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/thalles_silva/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [1,31,23,21] vs. [1,30,22,21]\n\t [[Node: add = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](seg_vars/pool4/BiasAdd, conv2d_transpose)]]\n\t [[Node: ResizeBilinear/_65 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_331_ResizeBilinear\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# Initializer for new fc8 weights -- for two classes.\n",
    "vgg_fc8_weights_initializer = tf.variables_initializer(vgg_fc8_weights)\n",
    "\n",
    "# Initializer for adam variables\n",
    "optimization_variables_initializer = tf.variables_initializer(adam_optimizer_variables)\n",
    "\n",
    "model_vars = tf.variables_initializer(model_variables)\n",
    "\n",
    "# Create a saver.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run the initializers.\n",
    "    read_vgg_weights_except_fc8_func(sess)\n",
    "    sess.run(vgg_fc8_weights_initializer)\n",
    "    sess.run(optimization_variables_initializer)\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(model_vars)\n",
    "    \n",
    "    out, train_image, train_annotation = sess.run([upsampled_by_factor_16_logits, image_tensor, annotation_tensor],\n",
    "                                              feed_dict={is_training_placeholder: False})\n",
    "    print(out.shape)\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "    f.set_figheight(4)\n",
    "    f.set_figwidth(10)\n",
    "    ax1.imshow(train_image[0])\n",
    "    ax1.set_title('Input image')\n",
    "    probability_graph = ax2.imshow(np.dstack((train_annotation,)*3)*100)\n",
    "    ax2.set_title('Input Ground-Truth Annotation')\n",
    "    plt.show()\n",
    "    \n",
    "    for step in range(100):\n",
    "\n",
    "        _, train_loss, pred_np, probabilities_np, tmp = sess.run([train_step, cross_entropy_sum, pred, probabilities, update_op],\n",
    "                                        feed_dict={is_training_placeholder: True})\n",
    "        miou_np = sess.run(miou)\n",
    "        \n",
    "        pred_annotation = np.expand_dims(pred_np[0], axis=2).astype(float)\n",
    "        print(\"Train step:\", step, \"\\tTraing Loss:\", train_loss, \"\\tmIOU:\", miou_np)\n",
    "\n",
    "        cmap = plt.get_cmap('bwr')\n",
    "        f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True)\n",
    "        f.set_figheight(4)\n",
    "        f.set_figwidth(16)\n",
    "\n",
    "        ax1.imshow(np.dstack((pred_annotation,)*3)*100)\n",
    "        ax1.set_title('Predication')\n",
    "        probability_graph = ax2.imshow(np.dstack((train_annotation,)*3)*100)\n",
    "        ax2.set_title('Input Ground-Truth Annotation')\n",
    "        probability_graph = ax3.imshow(probabilities_np.squeeze()[:, :, 0])\n",
    "        ax3.set_title('Prediction probabilities')\n",
    "        plt.show()\n",
    "\n",
    "    sess.close()\n",
    "\n",
    "    summary_string_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
